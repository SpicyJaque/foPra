{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import re\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "# from bertopic.representation import KeyBERTInspired\n",
    "# from hdbscan import HDBSCAN\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from umap import UMAP\n",
    "\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "from textblob_de import TextBlobDE\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# import pickle\n",
    "import importlib\n",
    "import functions\n",
    "import requests\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "This cell contains the necessary imports and setups required for the project, including libraries for data manipulation, visualization, natural language processing, and API interaction. It also defines the file path and loads the DeepL API key for translation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "importlib.reload(functions)\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "parties = [\"CDU\", \"SPD\", \"FDP\", \"AFD\", \"LEFT\", \"GREENS\"]\n",
    "file_path = \"C:/Users/Jacob/OneDrive/uni/MA WiSoz/Semester III/Computational Social Sciences/foPra/data/\"\n",
    "\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(\"API Keys/DeepL.txt\", 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    api_key_deepl = file.read()\n",
    "\n",
    "url = 'https://api-free.deepl.com/v2/translate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "This cell initializes necessary variables, downloads required NLTK resources, and loads the DeepL API key for translation. It also sets up the API endpoint URL for DeepL and defines a list of political parties for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit Text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto_raw = pd.read_csv(\"data/parties/All_Manifestos.csv\")\n",
    "df_manifesto_tokenized = pd.read_csv(\"data/parties/All_Manifestos_tokenized.csv\")\n",
    "cmp_categories = pd.read_csv(file_path  + \"cmp_categories.csv\")\n",
    "# df_manifesto[\"year\"] = pd.to_numeric(df_manifesto[\"year\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "This cell loads the manifesto data and related categories from CSV files into pandas DataFrames. It prepares the data for further processing by reading the manifesto text, tokenized text, and category mappings. The `file_path` variable is used to construct the path for the category file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdu = df_manifesto_raw[df_manifesto_raw['party'] == 'CDU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to df_manifesto\n",
    "df_manifesto = process_manifesto_data(df_manifesto_raw)\n",
    "# df_manifesto_tokenized = process_manifesto_data(df_manifesto_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_entries_by_conditions(df):\n",
    "    \"\"\"\n",
    "    Drops rows from the dataframe where year > 1997 and cmp_code is NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered dataframe.\n",
    "    \"\"\"\n",
    "    return df[ ~ ((df['cmp_code'].isna()) & (df['year'] > 1997))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_manifesto with cmp_categories to add the description_md column\n",
    "df_manifesto = df_manifesto.merge(cmp_categories[['code', 'label', 'description_md']], \n",
    "                                  left_on='cmp_code', \n",
    "                                  right_on='code', \n",
    "                                  how='left')\n",
    "\n",
    "df_manifesto.rename(columns={\"label\": \"category\"}, inplace=True)\n",
    "\n",
    "# Drop the redundant 'code' column after the merge\n",
    "df_manifesto.drop(columns=['code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to df_manifesto to reduce noise\n",
    "# df_manifesto = drop_na_after_date(df_manifesto, 1997)\n",
    "df_manifesto = df_manifesto[ ~ ((df_manifesto['cmp_code'].isna()) & (df_manifesto['year'] > 1997))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto_final[\"sentiment\"] = df_manifesto_final[\"text\"].apply(lambda x: TextBlobDE(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TextBlobDE(df_manifesto_final[\"text\"][2000])\n",
    "test.sentiment.polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto.to_pickle('data\\df_manifesto.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
