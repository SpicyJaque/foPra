{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import re\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import importlib\n",
    "import functions\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "importlib.reload(functions)\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "parties = [\"CDU\", \"SPD\", \"FDP\", \"AFD\", \"LEFT\", \"GREENS\"]\n",
    "file_path = \"C:/Users/Jacob/OneDrive/uni/MA WiSoz/Semester III/Computational Social Sciences/foPra/data/\"\n",
    "\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(\"API Keys/DeepL.txt\", 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    api_key_deepl = file.read()\n",
    "\n",
    "url = 'https://api-free.deepl.com/v2/translate'\n",
    "\n",
    "# Load the DataFrame from a pickle file\n",
    "df_manifesto = pd.read_pickle('data\\df_manifesto.pkl') \n",
    "topics = pd.read_pickle('topics.pkl') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit Text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining different models\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_en = \"all-MiniLM-L6-v2\"\n",
    "embedding_model_multilingual1 = \"paraphrase-multilingual-mpnet-base-v2\"\n",
    "embedding_model_multilingual2 = \"distiluse-base-multilingual-cased-v1\"\n",
    "embedding_model_manifestoberta = \"manifesto-project/manifestoberta-xlm-roberta-56policy-topics-sentence-2024-1-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = KeyBERTInspired()\n",
    "cluster_model = KMeans(n_clusters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=40, \n",
    "    n_components=5, \n",
    "    min_dist=0.1, \n",
    "    metric='cosine', \n",
    "    random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=35, \n",
    "    # min_samples=10,\n",
    "    metric='euclidean', \n",
    "    prediction_data=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    [\"steuergeld\", \n",
    "    \"ausgaben\", \n",
    "    \"staatsausgaben\", \n",
    "    \"staatshaushalt\", \n",
    "    \"schulden\", \n",
    "    \"schuldenbremse\", \n",
    "    \"staatsschulden\", \n",
    "    \"defizit\", \n",
    "    \"haushalt\", \n",
    "    \"haushaltsdefizit\", \n",
    "    \"haushaltsausgleich\", \n",
    "    \"schwarze null\", \n",
    "    \"schuldenabbau\", \n",
    "    \"schuldenpolitik\", \n",
    "    \"schuldenkrise\", \n",
    "    \"schuldenlast\", \n",
    "    \"schuldenstand\", \n",
    "    \"schuldenquote\", \n",
    "    \"schuldenpolitisch\",\n",
    "    \"ausgaben erhöhen\",\n",
    "    \"ausgaben kürzen\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisiere und trainiere das BERTopic-Modell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the 'text' column contains only strings and handle NaN values\n",
    "df_manifesto[\"text\"] = df_manifesto[\"text\"].astype(str).fillna(\"\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    language=\"multilingual\"\n",
    "    , n_gram_range=(1,2)\n",
    "    , min_topic_size=20\n",
    "    # , top_n_words=20\n",
    "    , representation_model=representation_model\n",
    "    , embedding_model=embedding_model_multilingual2\n",
    "    , umap_model=umap_model\n",
    "    # , hdbscan_model=hdbscan_model\n",
    "    , seed_topic_list=seed_topic_list\n",
    "    )\n",
    "\n",
    "#topic_model = BERTopic(nr_topics=100, calculate_probabilities=True, embedding_model=\"all_MiniLM-L6-v2\")\n",
    "topics, probs = topic_model.fit_transform(df_manifesto[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto[\"topic\"] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto.to_pickle('Manifesto_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('171 172', 0.47052616),\n",
       " ('172 173', 0.46464276),\n",
       " ('177 178', 0.46311623),\n",
       " ('181 182', 0.45728236),\n",
       " ('173 174', 0.45439306),\n",
       " ('176 177', 0.45394817),\n",
       " ('182 183', 0.4422307),\n",
       " ('178 179', 0.44135106),\n",
       " ('188 189', 0.43885046),\n",
       " ('186 187', 0.43734956)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([83, 131, 156, 128, 102, 160, 152, 94, 142, 96], [0.18113098, 0.13463318, 0.10732683, 0.08718705, 0.086810865, 0.076846965, 0.07013552, 0.059989497, 0.03488005, 0.031079182])\n"
     ]
    }
   ],
   "source": [
    "topic_model.get_topic(80)\n",
    "related_terms = topic_model.find_topics(\"related\", top_n=10)\n",
    "print(related_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topic_model.get_topic(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Erstelle das `topics_over_time`-Objekt\n",
    "topics_over_time = topic_model.topics_over_time(df_manifesto[\"text\"], df_manifesto[\"date\"])\n",
    "#topic_model = BERTopic(representation_model=KeyBERTInspired())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topics_over_time to a file\n",
    "with open(\"topics_over_time.pkl\", \"wb\") as f:\n",
    "    pickle.dump(topics_over_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Method 1 - safetensors\n",
    "topic_model.save(file_path, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model_multilingual2)\n",
    "\n",
    "# Method 2 - pytorch\n",
    "#topic_model.save(file_path, serialization=\"pytorch\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n",
    "# Method 3 - pickle\n",
    "#topic_model.save(\"bertopic_model.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = model.config.id2label[logits.argmax().item()]\n",
    "print(predicted_class)\n",
    "# 201 - Freedom and Human Rights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414 - Economic Orthodoxy\n"
     ]
    }
   ],
   "source": [
    "predicted_class = model.config.id2label[logits.argmax().item()]\n",
    "print(predicted_class)\n",
    "# 501 - Environmental Protection: Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topics to a pickle file\n",
    "with open('topics.pkl', 'wb') as f:\n",
    "\tpickle.dump(topics, f)\n",
    "\n",
    "# Save the probabilities to a pickle file\n",
    "with open('probs.pkl', 'wb') as f:\n",
    "\tpickle.dump(probs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
