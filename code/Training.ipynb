{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.functional.classification import accuracy, f1_score, precision, recall\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test, if running the model on either cpu or gpu is possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA!')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the dataframe with the manually labeled sentences. the _unlabeled/ _labeled file structure is a workaround to prevent overwriting the manually edited .csv file, if new labels are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto = pd.read_pickle('data\\df_manifesto_final.pkl')\n",
    "new_labels = pd.read_csv('data/df_spendings_unlabeled.csv', sep=';', encoding='utf-8-sig', index_col=0)\n",
    "old_labels = pd.read_csv('data/df_spendings_labeled.csv', sep=';', encoding='utf-8-sig', index_col=0)\n",
    "\n",
    "\n",
    "manual_labels = pd.concat([old_labels, new_labels], ignore_index=False)\n",
    "manual_labels = manual_labels.drop(columns=[\"description_md\"], errors=\"ignore\")\n",
    "manual_labels = manual_labels.dropna()\n",
    "\n",
    "manual_labels.to_csv('data/df_spendings_labeled.csv', sep=';', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_na_count = df_manifesto[\"label\"].notna().sum()\n",
    "random_samples = df_manifesto[df_manifesto[\"label\"].isna()].sample(non_na_count, random_state=30).index\n",
    "df_spendings = df_manifesto.copy()\n",
    "df_spendings.loc[random_samples, \"label\"] = 0\n",
    "df_spendings = df_spendings.dropna(subset=[\"label\"])\n",
    "df_spendings[\"label\"] = df_spendings[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet prepares data and initializes a pre-trained sentence embedding model for a classification task.\n",
    "\n",
    "1. **Extracting Sentences and Labels**:  \n",
    "   The variables `sentences` and `labels` are populated from the `df_spendings` DataFrame. Specifically:\n",
    "   - `df_spendings[\"text\"].tolist()` extracts the text data (assumed to be sentences or documents) from the \"text\" column and converts it into a Python list.\n",
    "   - `df_spendings[\"label\"].values` retrieves the corresponding labels from the \"label\" column as a NumPy array. These labels likely represent the target classes for the classification task.\n",
    "\n",
    "2. **Defining the Number of Classes**:  \n",
    "   The variable `num_classes` is set to 3, indicating that the classification task involves three distinct categories. This value will likely be used later in the model architecture or evaluation.\n",
    "\n",
    "3. **Loading the Sentence Transformer Model**:  \n",
    "   The `SentenceTransformer` class is used to load a pre-trained model, specifically `'sentence-transformers/distiluse-base-multilingual-cased-v2'`. This model is a multilingual version of DistilUSE (Distilled Universal Sentence Encoder) and is designed to generate high-quality sentence embeddings for a wide range of languages. Sentence embeddings are dense vector representations of sentences that capture their semantic meaning.\n",
    "\n",
    "4. **Moving the Model to the Device**:  \n",
    "   The `.to(device)` method moves the model to the specified computation device (`device`), which is typically either a GPU or CPU. This ensures that the model's computations are performed on the appropriate hardware for efficiency.\n",
    "\n",
    "In summary, this code prepares the input data (sentences and labels) and initializes a pre-trained sentence embedding model for multilingual text processing. The embeddings generated by this model will likely be used as input features for a downstream classification task involving three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = df_spendings[\"text\"].tolist(), df_spendings[\"label\"].values\n",
    "num_classes = 3\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating embeddings for a set of sentences using a pre-trained model and saving these embeddings, along with corresponding labels, to disk for later use.\n",
    "\n",
    "The `with torch.no_grad():` block is used to disable gradient computation in PyTorch. This is important here because the operation involves encoding sentences into embeddings, which is a forward pass through the model. Since no training or backpropagation is required, disabling gradient computation reduces memory usage and speeds up the process.\n",
    "\n",
    "Inside the block, the `model.encode` method is called to generate embeddings for the input `sentences`. The `convert_to_tensor=True` argument ensures that the input sentences are converted into PyTorch tensors before being processed by the model. The resulting embeddings are then moved to the CPU using `.cpu()` and converted to a NumPy array with `.numpy()`. This conversion is necessary because the embeddings will be saved in a format compatible with NumPy's file-saving utilities.\n",
    "\n",
    "The `np.save` function is used to save the embeddings and labels as `.npy` files in the data directory. Specifically:\n",
    "- `\"data\\embeddings.npy\"` stores the sentence embeddings, which are numerical representations of the input sentences in a high-dimensional space.\n",
    "- `\"data\\labels.npy\"` stores the corresponding labels, which are assumed to be defined elsewhere in the code.\n",
    "\n",
    "Saving these files allows for efficient reuse of the embeddings and labels without needing to recompute them, which is particularly useful when working with large datasets or computationally expensive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "np.save(\"data\\embeddings.npy\", embeddings)\n",
    "np.save(\"data\\labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\"data\\embeddings.npy\")\n",
    "labels = np.load(\"data\\labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset class is a custom implementation for managing data, likely intended for use with PyTorch's DataLoader. It takes two inputs during initialization: embeddings and labels. The embeddings represent the feature vectors for the data points, while labels are the corresponding target values (e.g., class labels for classification).\n",
    "\n",
    "This class encapsulates the logic for a simple feedforward neural network classifier, making it reusable and modular. Together, these two classes form the foundation for a machine learning pipeline, where the Dataset class handles data preparation and the ClassifierHead class defines the model architecture.\n",
    "\n",
    "The __len__ method returns the number of data points in the dataset, which is determined by the length of the labels list.\n",
    "The __getitem__ method retrieves a single data point and its label based on the provided index (idx). It converts the embeddings and labels at the specified index into PyTorch tensors with appropriate data types (float32 for embeddings and int64 for labels). This ensures compatibility with PyTorch models and training pipelines.\n",
    "This class is essential for preparing data in a format that can be efficiently processed during training or evaluation.\n",
    "\n",
    "The ClassifierHead class defines a neural network module for classification tasks. It inherits from PyTorch's nn.Module and is designed to process input feature vectors and produce predictions for two classes (binary classification).\n",
    "\n",
    "The constructor (__init__) initializes a sequential model (nn.Sequential) consisting of:\n",
    "\n",
    "A fully connected layer (nn.Linear) that maps the input features to 1024 dimensions.\n",
    "A ReLU activation function (nn.ReLU) to introduce non-linearity.\n",
    "A dropout layer (nn.Dropout) with a dropout rate of 0.2 to reduce overfitting by randomly zeroing some of the activations during training.\n",
    "Another fully connected layer that maps the 1024-dimensional features to 2 output dimensions, corresponding to the two classes.\n",
    "The forward method defines how the input data flows through the network. It takes an input tensor x, passes it through the sequential layers, and returns the output tensor out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embeddings = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.int64)\n",
    "        return embeddings, labels\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, num_classes), # Hier dann num_classes statt 2.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.classifier(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided function, model_pass, is a utility function designed to handle a single pass (either training or evaluation) of a machine learning model. It takes several arguments: model (the neural network to be trained or evaluated), criterion (the loss function), loader (a data loader providing batches of data), optimizer (used for updating model parameters during training), and train (a boolean indicating whether the pass is for training or evaluation).\n",
    "\n",
    "The function begins by setting the model's mode using model.train() or model.eval(), depending on whether the train flag is True or False. This ensures that the model behaves appropriately, such as enabling or disabling dropout layers during training or evaluation, respectively.\n",
    "\n",
    "It initializes empty lists to track losses, predictions (preds), and targets (targs). The function then iterates over the data loader, which provides batches of features and labels. These are moved to the appropriate device (e.g., CPU or GPU) for computation. The torch.set_grad_enabled(train) context manager ensures that gradients are only computed during training, saving memory and computation during evaluation.\n",
    "\n",
    "Within the loop, the model processes the input features to produce outputs, and the loss is computed using the provided criterion. If the pass is for training, the optimizer is used to update the model's parameters: gradients are computed via loss.backward(), and the optimizer steps forward with optimizer.step(). Predictions are obtained by taking the index of the maximum value along the output's last dimension (torch.argmax), and both predictions and labels are detached from the computation graph to avoid unnecessary memory usage.\n",
    "\n",
    "After processing all batches, the predictions and targets are concatenated into single tensors using torch.cat. The function then computes several evaluation metrics: accuracy, F1 score, precision, and recall, using helper functions like binary_accuracy, binary_f1_score, etc. These metrics are calculated for binary classification tasks, where the model predicts one of two possible classes.\n",
    "\n",
    "Finally, the function returns a dictionary containing the average loss and the computed metrics. This structure makes it easy to monitor the model's performance during training and evaluation, providing insights into how well the model is learning and generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pass(num_classes, model, criterion, loader, optimizer=None, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    preds = []\n",
    "    targs = []\n",
    "\n",
    "    for features, labels in loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds.append(pred.detach())\n",
    "        targs.append(labels.detach())\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "    targs = torch.cat(targs)\n",
    "\n",
    "    acc = accuracy(preds, targs, task=\"multiclass\", num_classes=num_classes)\n",
    "    f1 = f1_score(preds, targs, task=\"multiclass\", num_classes=num_classes)\n",
    "    prec = precision(preds, targs, task=\"multiclass\", num_classes=num_classes)\n",
    "    rec = recall(preds, targs, task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    return {\n",
    "        \"loss\": np.mean(losses),\n",
    "        \"accuracy\": acc.item(),\n",
    "        \"f1\": f1.item(),\n",
    "        \"precision\": prec.item(),\n",
    "        \"recall\": rec.item(),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up the training pipeline for a binary classification task using PyTorch. It begins by instantiating the ClassifierHead model with an input dimension of 512, which corresponds to the size of the feature vectors (embeddings). The model is moved to the appropriate device (e.g., CPU or GPU) for computation.\n",
    "\n",
    "The dataset is split into training and validation sets using the train_test_split function. The test_size=0.1 parameter ensures that 10% of the data is reserved for validation, while the remaining 90% is used for training. The stratify=labels argument ensures that the class distribution in the training and validation sets matches the original dataset, which is important for imbalanced datasets. The random_state=42 ensures reproducibility by fixing the random seed.\n",
    "\n",
    "To handle class imbalance, the compute_class_weight function calculates weights for each class based on their frequency in the training data. These weights are then used to create sample weights for each training example, ensuring that underrepresented classes are sampled more frequently during training. A WeightedRandomSampler is used in the DataLoader for the training dataset to implement this sampling strategy. The validation dataset, on the other hand, is loaded with a simple shuffle mechanism.\n",
    "\n",
    "The Dataset class is used to wrap the training and validation data, making it compatible with PyTorch's DataLoader. The DataLoader batches the data and, in the case of the training set, uses the weighted sampler to draw 500 samples per epoch with replacement. The batch size for both loaders is set to 64.\n",
    "\n",
    "The training process is configured to run for 250 epochs. The loss function used is CrossEntropyLoss, which is suitable for multi-class classification tasks (including binary classification). The optimizer is AdamW, a variant of the Adam optimizer that includes weight decay for better regularization. The learning rate is set to 1e-4. A learning rate scheduler, CosineAnnealingLR, is also defined to gradually reduce the learning rate over the course of training, with a minimum value of 1e-6. This helps the model converge more effectively by fine-tuning the learning rate as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "classifier = ClassifierHead(input_dim=512, num_classes=num_classes).to(device)\n",
    "\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.1, random_state=11, shuffle=True, stratify=labels\n",
    ")\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
    "train_sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "\n",
    "train_dataset = Dataset(train_features, train_labels)\n",
    "valid_dataset = Dataset(valid_features, valid_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    sampler=torch.utils.data.WeightedRandomSampler(\n",
    "        train_sample_weights, \n",
    "        500,\n",
    "        replacement=True\n",
    "    ),\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=.15*1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train F1 0.8560 - Valid F1 0.7810: 100%|██████████| 300/300 [00:12<00:00, 23.56it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(epochs))\n",
    "\n",
    "best_f1 = 0\n",
    "for e in pbar:\n",
    "\n",
    "    train_metrics = model_pass(num_classes, classifier, criterion, train_loader, optimizer, train=True)\n",
    "    valid_metrics = model_pass(num_classes, classifier, criterion, valid_loader, optimizer=None, train=False)\n",
    "    scheduler.step()\n",
    "\n",
    "    if valid_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = valid_metrics[\"f1\"]\n",
    "        torch.save(classifier.state_dict(), \"classifier.pt\")\n",
    "    \n",
    "    pbar.set_description(f\"Train F1 {train_metrics['f1']:.4f} - Valid F1 {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet implements a training loop for a machine learning model, with a focus on tracking performance metrics and saving the best-performing model based on validation F1 score. Here's a detailed explanation:\n",
    "\n",
    "1. **Progress Bar Initialization**:  \n",
    "   The `tqdm` library is used to create a progress bar (`pbar`) that iterates over the specified number of epochs. This provides a visual indicator of the training progress, making it easier to monitor.\n",
    "\n",
    "2. **Tracking the Best F1 Score**:  \n",
    "   The variable `best_f1` is initialized to 0. It is used to keep track of the highest F1 score achieved on the validation set during training. This ensures that the best-performing model is saved.\n",
    "\n",
    "3. **Training and Validation Loop**:  \n",
    "   For each epoch, the following steps are performed:\n",
    "   - **Training Pass**: The `model_pass` function is called with the training data loader (`train_loader`) and the optimizer. This function computes the loss, accuracy, F1 score, precision, and recall for the training set while updating the model's weights.\n",
    "   - **Validation Pass**: The `model_pass` function is called again, this time with the validation data loader (`valid_loader`) and no optimizer. This evaluates the model's performance on the validation set without updating its weights.\n",
    "\n",
    "4. **Learning Rate Adjustment**:  \n",
    "   The `scheduler.step()` method is called to adjust the learning rate based on the training progress. This is typically used to improve convergence.\n",
    "\n",
    "5. **Saving the Best Model**:  \n",
    "   If the F1 score from the validation pass (`valid_metrics[\"f1\"]`) exceeds the current `best_f1`, the model's state dictionary is saved to a file named `\"classifier.pt\"`. This ensures that the best-performing model is preserved for later use.\n",
    "\n",
    "6. **Updating the Progress Bar Description**:  \n",
    "   The `pbar.set_description` method is used to dynamically update the progress bar's description with the current training F1 score and the best validation F1 score. This provides real-time feedback on the model's performance during training.\n",
    "\n",
    "Overall, this code combines training, validation, and model checkpointing into a single loop, ensuring efficient and organized model training while tracking key performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_policy_stance(sentences):\n",
    "    if not isinstance(sentences, list):\n",
    "        sentences = [sentences]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(sentences, convert_to_tensor=True).to(device)\n",
    "\n",
    "    logits = classifier(embeddings)\n",
    "    label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    if label ==2:\n",
    "        return -1\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model by comparing the prediction label to the target label. In this case, pro-expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Wir haben deshalb den Erwerb von Wohneigentum z B durch die Eigenheimrente WohnRiester unterstützt' → 1\n",
      "'So erhöhen wird die Kaufkraft und stärken den Binnenmarkt Vgl Kapitel I Gute Arbeit' → 1\n",
      "'Das sind über 14  des Bruttoinlandsprodukts BIP' → 1\n",
      "'Damit der Konsum steigt müssen die Menschen mehr verfügbares Einkommen haben das  sie für zusätzlichen Konsum ausgeben können' → 1\n",
      "'Statt Dienstleistungen zu privatisieren und einzuschränken wollen wir dass öffentliche und soziale Leistungen ausgebaut werden  in Schulen und Hochschulen Pflege Betreuungsund Kultureinrichtungen öffentlichem Nahverkehr und im Umweltschutz' → 1\n",
      "'Wer diesen Weg verfolgt setzt unsere Zukunft aufs Spiel oder will harte Einschnitte in den Sozialstaat' → 1\n",
      "'Teilhaben Einmischen Zukunft schaffen  das beschreibt einen neuen Weg aus den Krisen und den Aufbruch hin zu einer offenen modernen Gesellschaft und einer Wirtschaft die besser und sparsamer mit unseren natürlichen Ressourcen umgeht' → 1\n",
      "'Bei Produkten und Dienstleistungen für Kinder sowie arbeitsintensiven HandwerksDienstleistungen wollen wir uns dafür einsetzen dass bei den EUVorgaben die entsprechenden Anwendungsbereiche für den ermäßigten Umsatzsteuersatz ausgeweitet werden  Der Steuervollzug auf Länderebene muss verbessert insbesondere muss mehr Fachpersonal eingestellt und der Steuervollzug bundesweit vereinheitlicht werden' → 1\n",
      "'Es muss durch eine nachhaltige öffentliche Investitionspolitik durch eine sozialökologische Neuausrichtung der Wirtschaftspolitik durch Schaffung guter Arbeit und durch Beschäftigungspolitik gelöst werden' → 1\n",
      "'Wir Freie Demokraten fordern für den Fall der Fälle bei Wirtschaftshilfen und Hilfen für Selbstständige in Zukunft besser gerüstet zu sein' → 1\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "new_sentences = df_manifesto[df_manifesto['label'] == 1]['text'].sample(10).tolist()\n",
    "\n",
    "for sentence in new_sentences:\n",
    "    print(f\"'{sentence}' → {predict_policy_stance(sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function calculates the ratio of correctly predicted statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_match_ratio(df):\n",
    "    # Check if 'label' equals 'predicted_label'\n",
    "    matches = df['label'] == df['predicted_label']\n",
    "    \n",
    "    # Count 'yes' (True) and 'no' (False)\n",
    "    yes_count = matches.sum()\n",
    "    \n",
    "    # Calculate the ratio of 'yes' to 'no'\n",
    "    ratio = yes_count/len(df)\n",
    "    \n",
    "    return ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8366666666666667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df_manifesto.sample(300)\n",
    "sample['predicted_label'] = sample['text'].apply(predict_policy_stance)\n",
    "sample['label'] = sample['label'].apply(lambda label: 0 if pd.isna(label) else (-1 if label == 2 else label))\n",
    "\n",
    "calculate_label_match_ratio(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply the model to the entire dataset to predict each policy statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto['predicted_label'] = df_manifesto['text'].apply(predict_policy_stance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edit label column to match the prediction classification scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto[\"label\"]=df_manifesto[\"label\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto[\"label\"] = df_manifesto[\"label\"].replace(2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto.to_pickle('data/df_manifesto_predictions.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
