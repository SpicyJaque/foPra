{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import re\n",
    "\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "import importlib\n",
    "import functions\n",
    "import requests\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "importlib.reload(functions)\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "parties = [\"CDU\", \"SPD\", \"FDP\", \"AFD\", \"LEFT\", \"GREENS\"]\n",
    "file_path = \"C:/Users/Jacob/OneDrive/uni/MA WiSoz/Semester III/Computational Social Sciences/foPra/data/\"\n",
    "\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(\"API Keys/DeepL.txt\", 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    api_key_deepl = file.read()\n",
    "\n",
    "url = 'https://api-free.deepl.com/v2/translate'\n",
    "df_manifesto = pd.read_csv(\"data/parties/Manifestos_tokenized.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'auth_key': api_key,\n",
    "    'text': 'Hello, world!',\n",
    "    'target_lang': 'DE'  # Translate to German\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the API request\n",
    "response = requests.post(url, data=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    translated_text = result['translations'][0]['text']\n",
    "    print(f'Translated text: {translated_text}')\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit Text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining different models\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=50, \n",
    "    # n_components=5, \n",
    "    # min_dist=0.0, \n",
    "    metric='cosine', \n",
    "    random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_en = \"all-MiniLM-L6-v2\"\n",
    "embedding_model_multilingual1 = \"paraphrase-multilingual-mpnet-base-v2\"\n",
    "embedding_model_multilingual2 = \"distiluse-base-multilingual-cased-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = KeyBERTInspired()\n",
    "cluster_model = KMeans(n_clusters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50, \n",
    "    # min_samples=10,\n",
    "    metric='euclidean', \n",
    "    prediction_data=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [[\"steuergeld\", \"ausgaben\", \"staatsausgaben\", \"staatshaushalt\", \"schulden\", \"schuldenbremse\", \"staatsschulden\"],\n",
    "                   ['sozial wohnungsbau', 'sozialen wohnungsbaus', 'sozialen wohnungsbau', 'sozialwohnungen', 'wohnungsbauförderung', 'mietwohnungen', 'wohnungsbaus', 'bezahlbar wohnungen', 'eigentumswohnungen', 'wohnungsbau'],\n",
    "                   ['öffentlich investitionen', 'öffentlichen investitionen', 'investitionen öffentlichen', 'investitionen', 'investitionsquot', 'investitionsstau', 'investitionszulag', 'investitionsmittel', 'privat investitionen', 'investitionsanr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisiere und trainiere das BERTopic-Modell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the 'text' column contains only strings and handle NaN values\n",
    "df_manifesto[\"text\"] = df_manifesto[\"text\"].astype(str).fillna(\"\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    language=\"multilingual\"\n",
    "    , n_gram_range=(1,2)\n",
    "    # , min_topic_size=20\n",
    "    # , top_n_words=20\n",
    "    # , representation_model=representation_model\n",
    "    , embedding_model=embedding_model_multilingual2\n",
    "    , umap_model=umap_model\n",
    "    # , hdbscan_model=hdbscan_model\n",
    "    , seed_topic_list=seed_topic_list\n",
    "    )\n",
    "\n",
    "#topic_model = BERTopic(nr_topics=100, calculate_probabilities=True, embedding_model=\"all_MiniLM-L6-v2\")\n",
    "topics, probs = topic_model.fit_transform(df_manifesto[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto[\"topic\"] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manifesto.to_pickle('Manifesto_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topic_model.get_topic(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Erstelle das `topics_over_time`-Objekt\n",
    "topics_over_time = topic_model.topics_over_time(df_manifesto[\"text\"], df_manifesto[\"date\"])\n",
    "#topic_model = BERTopic(representation_model=KeyBERTInspired())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topics_over_time to a file\n",
    "with open(\"topics_over_time.pkl\", \"wb\") as f:\n",
    "    pickle.dump(topics_over_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Method 1 - safetensors\n",
    "topic_model.save(file_path[:-5], serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model_multilingual2)\n",
    "\n",
    "# Method 2 - pytorch\n",
    "#topic_model.save(file_path, serialization=\"pytorch\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n",
    "# Method 3 - pickle\n",
    "#topic_model.save(\"bertopic_model.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topics to a pickle file\n",
    "with open('topics.pkl', 'wb') as f:\n",
    "\tpickle.dump(topics, f)\n",
    "\n",
    "# Save the probabilities to a pickle file\n",
    "with open('probs.pkl', 'wb') as f:\n",
    "\tpickle.dump(probs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
