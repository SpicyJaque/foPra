{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob_de import TextBlobDE\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import importlib\n",
    "import functions\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from a pickle file\n",
    "df_manifesto = pd.read_pickle('data/df_manifesto_final.pkl') \n",
    "df_spendings = pd.read_pickle('data/df_spendings_final.pkl')\n",
    "# df_no_label = pd.read_csv('data/Unlabeled.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings[\"sentiment\"] = df_spendings[\"text\"].apply(lambda x: TextBlobDE(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df_spendings[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_spendings['sentiment'] = sentiment\n",
    "df_spendings = df_spendings[df_spendings['sentiment'] != 0]\n",
    "df_spendings = df_spendings[df_spendings['topic'] != 57]\n",
    "df_spendings = df_spendings.drop(columns=['description_md'])\n",
    "df_spendings.to_csv('data/df_spendings_final.csv', index=True, encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_spendings.sample(100)  # Replace 5 with the desired number of random rows\n",
    "sample = sample[['text', 'cmp_code', 'label'] + [col for col in sample.columns if col not in ['text', 'cmp_code', 'label']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_spendings['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_spendings[\"text\"].tolist(), df_spendings[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size of the training and validation datasets\n",
    "small_train_dataset = dataset.select(range(100))  # Use the first 100 samples\n",
    "small_val_dataset = val_dataset.select(range(50))  # Use the first 50 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at manifesto-project/manifestoberta-xlm-roberta-56policy-topics-context-2024-1-1 and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([56]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([56, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069d338eff164b0895ae988b988327d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"manifesto-project/manifestoberta-xlm-roberta-56policy-topics-context-2024-1-1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\", use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "model.config.id2label = {0: \"expansion\", 1: \"austerity\"}\n",
    "model.config.label2id = {\"expansion\": 0, \"austerity\": 1}\n",
    "model.config.language = \"german\"\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df_spendings)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = hf_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size of the training and validation datasets\n",
    "small_train_dataset = train_dataset.select(range(100))  # Use the first 100 samples\n",
    "small_val_dataset = val_dataset.select(range(50))  # Use the first 50 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/Jacob/Desktop/Code/fopra/output_dir\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1000,  # Save a checkpoint every 1000 steps\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,  # Reduce batch size for CPU\n",
    "    gradient_accumulation_steps=4,  # Increase to simulate larger batch size\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory to avoid clogging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"Using device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_split = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    # no_cuda=True,  # Force training on CPU\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimum: this way the model works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_opt = TrainingArguments(\n",
    "    output_dir=\"C:/Users/Jacob/Desktop/Code/fopra/output_dir\",\n",
    "    evaluation_strategy=\"steps\",  # Evaluate more frequently\n",
    "    eval_steps=500,  # Evaluate every 500 steps\n",
    "    save_steps=500,  # Save a checkpoint every 500 steps\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    learning_rate=1e-5,  # Smaller learning rate for better precision\n",
    "    per_device_train_batch_size=8,  # Increase batch size if memory allows\n",
    "    gradient_accumulation_steps=1,  # Keep this if memory is limited\n",
    "    num_train_epochs=5,  # Train for more epochs\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True,  # Force training on CPU\n",
    "    overwrite_output_dir=True,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    metric_for_best_model=\"accuracy\",  # Use accuracy as the metric\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Trainer\n",
    "trainer_small = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_val_dataset,\n",
    "    tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer_small.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_policy_stance(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    return \"expansion\" if predicted_class == 0 else \"austerity\"\n",
    "\n",
    "# Example predictions\n",
    "new_sentences = [\n",
    "    \"wir wollen die Schuldenbremse abschaffen.\",\n",
    "    \"wir wollen mehr geld für den sozialstaat ausgeben\"\n",
    "]\n",
    "\n",
    "for sentence in new_sentences:\n",
    "    print(f\"'{sentence}' → {predict_policy_stance(sentence)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
